{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/utkarsh.verma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "                            ### Data for the model ###\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "review1 = pd.read_csv('sentiment-analysis-on-movie-reviews/train.csv')\n",
    "\n",
    "\n",
    "#review = review1.iloc[0:4000] \n",
    "#test = review1.iloc[4001:4200] \n",
    "\n",
    "\n",
    "\n",
    "label=np.asarray(review1.Sentiment, dtype=int)\n",
    "#x_train=review.Phrase\n",
    "\n",
    "unique_elements, counts_elements = np.unique(label, return_counts=True)\n",
    "#min_value=np.min(counts_elements)\n",
    "min_value = 400\n",
    "\n",
    "df0 = review1.loc[review1['Sentiment'] == 0].sample(min_value)\n",
    "df0_train = df0.sample(int(min_value*0.8))\n",
    "df0_test = df0.sample(int(min_value*0.2))\n",
    "\n",
    "df1=review1.loc[review1['Sentiment'] == 1].sample(min_value)\n",
    "df1_train = df1.sample(int(min_value*0.8))\n",
    "df1_test = df1.sample(int(min_value*0.2))\n",
    "\n",
    "df2=review1.loc[review1['Sentiment'] == 2].sample(min_value)\n",
    "df2_train = df2.sample(int(min_value*0.8))\n",
    "df2_test = df2.sample(int(min_value*0.2))\n",
    "\n",
    "df3=review1.loc[review1['Sentiment'] == 3].sample(min_value)\n",
    "df3_train = df3.sample(int(min_value*0.8))\n",
    "df3_test = df3.sample(int(min_value*0.2))\n",
    "\n",
    "df4=review1.loc[review1['Sentiment'] == 4].sample(min_value)\n",
    "df4_train = df4.sample(int(min_value*0.8))\n",
    "df4_test = df4.sample(int(min_value*0.2))\n",
    "\n",
    "balanced_df_train = pd.concat([df0_train, df1_train,df2_train,df3_train,df4_train], axis=0)\n",
    "balanced_df_test = pd.concat([df0_test, df1_test,df2_test,df3_test,df4_test], axis=0)\n",
    "\n",
    "y_train=np.asarray(balanced_df_train.Sentiment, dtype=int)\n",
    "x_train=np.asarray(balanced_df_train.Phrase)\n",
    "\n",
    "y_test=np.asarray(balanced_df_test.Sentiment, dtype=int)\n",
    "x_test=np.asarray(balanced_df_test.Phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "review1 = pd.read_csv('sentiment-analysis-on-movie-reviews/train_emoji.csv')\n",
    "y_train=np.asarray(review1.Sentiment, dtype=int)\n",
    "x_train=np.asarray(review1.Tweet)\n",
    "review1 = pd.read_csv('sentiment-analysis-on-movie-reviews/test_emoji.csv')\n",
    "y_test=np.asarray(review1.Sentiment, dtype=int)\n",
    "x_test=np.asarray(review1.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = pd.read_csv('sentiment-analysis-on-movie-reviews/train.csv').iloc[0:4000]\n",
    "y_train=np.asarray(review1.Sentiment, dtype=int)\n",
    "x_train=np.asarray(review1.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            ### Preprocessing the data\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def preprocess(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    corpus = []\n",
    "    \n",
    "    review_text = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    review_text = review_text.lower()\n",
    "    review_text = review_text.split()\n",
    "    review_text = [ps.stem(word) for word in review_text if not word in set(stopwords.words('english'))]\n",
    "    #review_text = ' '.join(review_text)\n",
    "    #corpus.append(review_text)\n",
    "    return review_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        ### Frequency Dictionary  ###\n",
    "\n",
    "freq_Dict={}\n",
    "yt = np.squeeze(y_train).tolist()\n",
    "for sentence,y in zip(x_train,yt):\n",
    "    for word in preprocess(sentence):\n",
    "        if (word,y) in freq_Dict:\n",
    "            freq_Dict[(word,y)]+=1\n",
    "        else:\n",
    "            freq_Dict[(word,y)]=1 \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('proud', 2): 2,\n",
       " ('achiev', 2): 1,\n",
       " ('worst', 3): 1,\n",
       " ('day', 3): 1,\n",
       " ('life', 3): 1,\n",
       " ('miss', 0): 4,\n",
       " ('much', 0): 1,\n",
       " ('food', 4): 5,\n",
       " ('life', 4): 1,\n",
       " ('love', 0): 5,\n",
       " ('mum', 0): 1,\n",
       " ('stop', 3): 3,\n",
       " ('say', 3): 1,\n",
       " ('bullshit', 3): 1,\n",
       " ('congratul', 2): 2,\n",
       " ('accept', 2): 1,\n",
       " ('assign', 3): 1,\n",
       " ('long', 3): 1,\n",
       " ('want', 1): 1,\n",
       " ('go', 1): 2,\n",
       " ('play', 1): 3,\n",
       " ('answer', 3): 1,\n",
       " ('text', 3): 1,\n",
       " ('stupid', 3): 1,\n",
       " ('limit', 3): 1,\n",
       " ('mani', 1): 1,\n",
       " ('point', 1): 1,\n",
       " ('score', 1): 1,\n",
       " ('algorithm', 3): 1,\n",
       " ('perform', 3): 1,\n",
       " ('poorli', 3): 1,\n",
       " ('got', 2): 2,\n",
       " ('approv', 2): 1,\n",
       " ('shout', 3): 2,\n",
       " ('sound', 2): 1,\n",
       " ('like', 2): 2,\n",
       " ('fun', 2): 2,\n",
       " ('plan', 2): 1,\n",
       " ('ha', 2): 5,\n",
       " ('one', 3): 1,\n",
       " ('like', 3): 1,\n",
       " ('game', 1): 3,\n",
       " ('finish', 1): 1,\n",
       " ('celebr', 2): 1,\n",
       " ('soon', 2): 1,\n",
       " ('sad', 3): 1,\n",
       " ('come', 3): 2,\n",
       " ('dearest', 0): 1,\n",
       " ('good', 2): 2,\n",
       " ('job', 2): 4,\n",
       " ('funni', 2): 4,\n",
       " ('lol', 2): 2,\n",
       " ('candi', 2): 1,\n",
       " ('life', 2): 1,\n",
       " ('chicago', 1): 1,\n",
       " ('cub', 1): 1,\n",
       " ('hungri', 4): 1,\n",
       " ('excit', 2): 2,\n",
       " ('see', 2): 1,\n",
       " ('long', 2): 1,\n",
       " ('well', 2): 1,\n",
       " ('exam', 2): 1,\n",
       " ('let', 4): 2,\n",
       " ('brunch', 4): 1,\n",
       " ('day', 4): 1,\n",
       " ('cute', 0): 3,\n",
       " ('dare', 3): 1,\n",
       " ('ask', 3): 1,\n",
       " ('want', 4): 4,\n",
       " ('join', 4): 1,\n",
       " ('dinner', 4): 2,\n",
       " ('said', 2): 1,\n",
       " ('ye', 2): 1,\n",
       " ('attract', 0): 1,\n",
       " ('suck', 3): 1,\n",
       " ('smile', 2): 3,\n",
       " ('lot', 2): 2,\n",
       " ('laugh', 2): 1,\n",
       " ('take', 3): 1,\n",
       " ('forev', 3): 1,\n",
       " ('get', 3): 1,\n",
       " ('readi', 3): 1,\n",
       " ('french', 4): 1,\n",
       " ('macaroon', 4): 1,\n",
       " ('tasti', 4): 1,\n",
       " ('made', 2): 1,\n",
       " ('ador', 0): 2,\n",
       " ('dog', 0): 3,\n",
       " ('girl', 3): 1,\n",
       " ('mean', 3): 1,\n",
       " ('two', 0): 1,\n",
       " ('code', 3): 1,\n",
       " ('work', 3): 2,\n",
       " ('grader', 3): 1,\n",
       " ('gave', 3): 1,\n",
       " ('zero', 3): 1,\n",
       " ('joke', 2): 3,\n",
       " ('kill', 2): 1,\n",
       " ('haha', 2): 1,\n",
       " ('like', 4): 1,\n",
       " ('pizza', 4): 1,\n",
       " ('got', 3): 2,\n",
       " ('grade', 3): 1,\n",
       " ('think', 3): 1,\n",
       " ('end', 3): 1,\n",
       " ('alon', 3): 1,\n",
       " ('humili', 3): 1,\n",
       " ('sister', 3): 1,\n",
       " ('aw', 3): 1,\n",
       " ('cook', 4): 1,\n",
       " ('meat', 4): 1,\n",
       " ('let', 1): 1,\n",
       " ('exercis', 1): 1,\n",
       " ('best', 1): 1,\n",
       " ('player', 1): 2,\n",
       " ('stadium', 1): 2,\n",
       " ('incred', 2): 1,\n",
       " ('intellig', 2): 1,\n",
       " ('talent', 2): 1,\n",
       " ('favorit', 1): 1,\n",
       " ('like', 0): 1,\n",
       " ('lot', 0): 1,\n",
       " ('puppi', 0): 1,\n",
       " ('hate', 3): 1,\n",
       " ('chines', 4): 1,\n",
       " ('cooki', 4): 1,\n",
       " ('good', 4): 1,\n",
       " ('charm', 2): 1,\n",
       " ('bravo', 2): 2,\n",
       " ('announc', 2): 1,\n",
       " ('traction', 2): 1,\n",
       " ('basebal', 1): 4,\n",
       " ('amaz', 2): 1,\n",
       " ('babi', 0): 1,\n",
       " ('wait', 3): 1,\n",
       " ('two', 3): 1,\n",
       " ('hour', 3): 1,\n",
       " ('peopl', 2): 1,\n",
       " ('kind', 0): 1,\n",
       " ('friendli', 0): 1,\n",
       " ('bad', 3): 3,\n",
       " ('cannot', 3): 1,\n",
       " ('us', 3): 1,\n",
       " ('like', 1): 1,\n",
       " ('impress', 2): 1,\n",
       " ('dedic', 2): 1,\n",
       " ('project', 2): 1,\n",
       " ('moment', 2): 1,\n",
       " ('sushi', 4): 1,\n",
       " ('disappoint', 3): 1,\n",
       " ('anyth', 3): 1,\n",
       " ('togeth', 4): 1,\n",
       " ('dear', 0): 1,\n",
       " ('look', 0): 1,\n",
       " ('date', 0): 1,\n",
       " ('frustrat', 3): 1,\n",
       " ('lost', 3): 1,\n",
       " ('wallet', 3): 1,\n",
       " ('fail', 3): 1,\n",
       " ('midterm', 3): 1,\n",
       " ('want', 0): 1,\n",
       " ('give', 0): 1,\n",
       " ('hug', 0): 1,\n",
       " ('final', 1): 1,\n",
       " ('happi', 2): 2,\n",
       " ('qualifi', 3): 1,\n",
       " ('posit', 3): 1,\n",
       " ('dad', 0): 1,\n",
       " ('guy', 2): 1,\n",
       " ('special', 2): 1,\n",
       " ('great', 2): 3,\n",
       " ('could', 3): 1,\n",
       " ('solv', 3): 1,\n",
       " ('congrat', 2): 1,\n",
       " ('new', 2): 1,\n",
       " ('forev', 2): 1,\n",
       " ('eat', 4): 1,\n",
       " ('catcher', 1): 1,\n",
       " ('suck', 1): 1,\n",
       " ('first', 1): 1,\n",
       " ('base', 1): 1,\n",
       " ('man', 1): 1,\n",
       " ('got', 1): 1,\n",
       " ('ball', 1): 2,\n",
       " ('homework', 3): 1,\n",
       " ('chees', 4): 1,\n",
       " ('cake', 4): 1,\n",
       " ('lectur', 2): 1,\n",
       " ('though', 2): 1,\n",
       " ('afternoon', 1): 1,\n",
       " ('rule', 1): 1,\n",
       " ('alway', 3): 1,\n",
       " ('cutest', 0): 1,\n",
       " ('person', 0): 1,\n",
       " ('ever', 0): 1,\n",
       " ('seen', 0): 1,\n",
       " ('veget', 4): 1,\n",
       " ('healthi', 4): 1,\n",
       " ('handsom', 0): 1,\n",
       " ('loser', 3): 1,\n",
       " ('love', 4): 1,\n",
       " ('indian', 4): 1,\n",
       " ('restaur', 4): 1,\n",
       " ('make', 1): 1,\n",
       " ('home', 1): 1,\n",
       " ('run', 1): 1,\n",
       " ('order', 4): 1,\n",
       " ('wrong', 3): 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sigmid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def value(key):\n",
    "    return freq_Dict[key] if key in freq_Dict else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### Gradient Descent for Logistic Regression  ###\n",
    "\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        z = np.dot(x,theta)\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h))+np.dot((1-y).T,np.log(1-h)))\n",
    "        theta = theta - (alpha/m)*np.dot(x.T,h-y)\n",
    "\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "                ### Feacture Extraction For different Class ###\n",
    "\n",
    "def extract_features(tweet, freq_Dict,Class):\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    array = np.array([0,1, 2, 3, 4])\n",
    "    array = np.delete(array, Class)\n",
    "\n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    for word in tweet:\n",
    "        \n",
    "        x[0,1] += freq_Dict[(word, Class)] if (word, Class) in freq_Dict else 0\n",
    "        \n",
    "        ind = tuple(zip(np.repeat(word, len(array)),array))\n",
    "        count = np.sum(list(map(value, ind)))/float(len(array))\n",
    "        x[0,2] += count\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y(data,Class):\n",
    "    y_train=[]\n",
    "    for v in data:\n",
    "        y_train.append(1 if v==Class else 0)\n",
    "    return np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Weights\n",
      "The cost after training is 0.37885446.\n",
      "The resulting vector of weights for class is [-0.22980513, 0.2107419, -0.43103598]\n",
      "Class 1 Weights\n",
      "The cost after training is 0.39997786.\n",
      "The resulting vector of weights for class is [-0.2112664, 0.27424492, -0.35764806]\n",
      "Class 2 Weights\n",
      "The cost after training is 0.36796971.\n",
      "The resulting vector of weights for class is [-0.12410651, 0.32493755, -0.28682953]\n",
      "Class 3 Weights\n",
      "The cost after training is 0.41277190.\n",
      "The resulting vector of weights for class is [-0.19856579, 0.25611999, -0.34644596]\n",
      "Class 4 Weights\n",
      "The cost after training is 0.36648818.\n",
      "The resulting vector of weights for class is [-0.24168815, 0.22265514, -0.43481782]\n"
     ]
    }
   ],
   "source": [
    "theta_all={}\n",
    "#X_train=preprocess(x_train)\n",
    "for Class in range(5):\n",
    "    \n",
    "    X = np.zeros((len(x_train), 3))\n",
    "    for i in range(len(x_train)):\n",
    "        X[i, :]= extract_features(preprocess(x_train[i]), freq_Dict,Class)\n",
    "        \n",
    "    Y = preprocess_y(y_train,Class).reshape(len(y_train),1)\n",
    "\n",
    "    # Apply gradient descent\n",
    "    J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 0.001, 1500)\n",
    "    print(\"Class \" +str(Class)+\" Weights\")\n",
    "    theta_all[\"Class \" + str(Class)]=theta\n",
    "    print(f\"The cost after training is {J:.8f}.\")\n",
    "    \n",
    "    print(f\"The resulting vector of weights for class is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freq_Dict, theta_all):\n",
    "    y_pred=[]\n",
    "    for Class in range(5):\n",
    "        x = extract_features(tweet, freq_Dict,Class)\n",
    "        y_pred.append(sigmoid(np.dot(x,theta_all[\"Class \" + str(Class)])))\n",
    "        \n",
    "    \n",
    "\n",
    "    return np.argmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_logistic_regression(test_x, test_y, freq_Dict, theta_all):\n",
    "\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(preprocess(tweet), freq_Dict, theta_all)\n",
    "        y_hat.append(y_pred)\n",
    "\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = np.count_nonzero(np.squeeze(np.asarray(y_hat))==np.squeeze(test_y))/len(test_y)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test1 = pd.read_csv('sentiment-analysis-on-movie-reviews/test_emoji.csv')\n",
    "review_test1[[\"Tweet\",\"Sentiment\"]]\n",
    "y_test=np.asarray(review_test1.Sentiment, dtype=int)\n",
    "x_test=np.asarray(review_test1.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=test_logistic_regression(x_test,y_test, freq_Dict, theta_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[key[0] for key in freq_Dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_per_class(Class):\n",
    "    array = np.array([0,1, 2, 3, 4])\n",
    "    array = np.delete(array, Class)\n",
    "    word_pn_Dict={}\n",
    "    \n",
    "    Dpos = np.count_nonzero(y_train==Class)\n",
    "    Dneg = len(y_train-Dpos)\n",
    "    \n",
    "    logprior = np.log(Dpos/Dneg)\n",
    "   \n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freq_Dict.keys():\n",
    "        if pair[1] == Class:\n",
    "            N_pos += freq_Dict[pair]\n",
    "\n",
    "        else:\n",
    "            N_neg += freq_Dict[pair]\n",
    "          \n",
    "    for word in vocab:\n",
    "        pos = freq_Dict[(word,Class)] if (word,Class) in freq_Dict else 0\n",
    "        \n",
    "        ind = tuple(zip(np.repeat(word, len(array)),array))\n",
    "        count = np.sum(list(map(value, ind)))\n",
    "        neg = count\n",
    "        \n",
    "        p_pos = (pos+1)/(N_pos+len(vocab))\n",
    "        p_neg = (neg+1)/(N_neg+len(vocab))\n",
    "        \n",
    "        word_pn_Dict[word]=np.log(p_pos/p_neg)\n",
    "    return logprior, word_pn_Dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_per_class(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logpriors, word_pn_Dicts):\n",
    "\n",
    "    word_l = tweet\n",
    "    y_pred=[]\n",
    "    for Class in range(5):\n",
    "    \n",
    "        p = 0\n",
    "\n",
    "        \n",
    "        p += logpriors[Class]\n",
    "\n",
    "        for word in word_l:\n",
    "\n",
    "            \n",
    "            if word in word_pn_Dicts[Class]:\n",
    "                \n",
    "                p += word_pn_Dicts[Class][word]\n",
    "        y_pred.append(p)\n",
    "\n",
    "    return np.argmax(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpriors=[]\n",
    "word_pn_Dicts=[]\n",
    "for i in range(5):\n",
    "    logprior, word_pn_Dict=dictionary_per_class(i)\n",
    "    logpriors.append(logprior)\n",
    "    word_pn_Dicts.append(word_pn_Dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logprior, word_pn_Dict=dictionary_per_class(3)\n",
    "naive_bayes_predict(X_train[2100], logpriors, word_pn_Dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " ###       Accuracy of naive bayes\n",
    "    \n",
    "def test_naive_bayes(test_x, test_y, logpriors, word_pn_Dicts):\n",
    " \n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x: \n",
    "        y_hat_i = naive_bayes_predict(preprocess(tweet), logpriors, word_pn_Dicts)\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    accuracy = np.count_nonzero(np.squeeze(np.asarray(y_hats))==np.squeeze(test_y))/len(test_y)\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    #accuracy = 1-error\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=test_naive_bayes(x_test,y_test, logpriors, word_pn_Dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
